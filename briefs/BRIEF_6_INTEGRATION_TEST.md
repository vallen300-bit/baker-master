# BRIEF 6 — End-to-End Integration Test

**Punch:** 6 (Validation Gate)
**Goal:** Prove the full Baker pipeline works as a chain — from trigger input to dashboard display and Slack delivery. Not unit tests. One script that exercises the live system and reports pass/fail for each link in the chain.

---

## Why This Exists

Punches 1–5 were built and tested in isolation:

| Punch | What it proved | What it didn't prove |
|-------|---------------|---------------------|
| 1–2 | Qdrant retrieval works | Retrieval → Augmentation handoff |
| 3 | Prompt builder assembles within budget | Builder output → Claude input chain |
| 4A–4D | Triggers fire, store_back writes to PostgreSQL | Trigger → Pipeline → Store → Output full loop |
| 5A | Slack notifier posts messages | Alerts generated by pipeline actually reach Slack |
| 5B | API serves JSON from PostgreSQL | Data written by pipeline appears in API responses |
| 5C | Frontend renders from API | Live pipeline data renders correctly |

**Brief 6 closes the loop.** One test script injects a synthetic trigger, watches it flow through all 5 steps, and verifies the output appears in both the REST API and Slack.

---

## Critical Directive

> **Test the live system — don't mock it.** This test connects to real PostgreSQL (Neon), real Qdrant, real Claude API, and real Slack webhook. It uses a clearly-marked synthetic trigger (`[INTEGRATION-TEST]` prefix) so test data is identifiable and cleanable. If any external service is unreachable, that specific test is marked SKIP (not FAIL) — infrastructure availability is not what we're testing.

---

## The Chain Under Test

```
[Synthetic Trigger]
       │
       ▼
┌─────────────────┐
│ 1. TRIGGER      │  TriggerEvent created with test content
│    pipeline.py   │
└────────┬────────┘
         ▼
┌─────────────────┐
│ 2. RETRIEVAL    │  retriever.retrieve_for_trigger()
│    retriever.py  │  → returns RetrievedContext[] from Qdrant + PostgreSQL
└────────┬────────┘
         ▼
┌─────────────────┐
│ 3. AUGMENTATION │  prompt_builder.build_prompt()
│    prompt_builder│  → returns system prompt + messages within token budget
└────────┬────────┘
         ▼
┌─────────────────┐
│ 4. GENERATION   │  Claude API call
│    pipeline.py   │  → returns SentinelResponse (alerts, decisions, analysis)
└────────┬────────┘
         ▼
┌─────────────────┐
│ 5. STORE BACK   │  store_back: PostgreSQL writes + Qdrant embedding
│    store_back.py │  + Slack delivery (Tier 1/2 alerts)
│    slack_notifier│  + Dashboard API serves new data
└─────────────────┘
```

---

## File to Create

### `scripts/test_integration.py` (~350 lines)

Single Python script. No test framework (no pytest) — just functions that return pass/fail. Runs top-to-bottom, prints a clear report.

**Structure:**

```python
"""
Baker Integration Test — End-to-End Pipeline Validation
Injects a synthetic trigger, traces it through all 5 steps,
verifies outputs in PostgreSQL, API, and Slack.

Usage:
    python scripts/test_integration.py
    python scripts/test_integration.py --skip-slack   # skip Slack delivery check
    python scripts/test_integration.py --cleanup-only  # remove test data only
"""

import sys, os, time, json, requests
from datetime import datetime, timezone

# Add parent to path so imports work
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# === CONFIGURATION ===
TEST_PREFIX = "[INTEGRATION-TEST]"
TEST_CONTACT = "Christophe Buchwalder"  # Real seeded contact — proves retrieval works
DASHBOARD_URL = "http://localhost:8080"
TIMEOUT = 120  # max seconds for pipeline run

# === TEST RESULTS TRACKER ===
results = []

def record(name, passed, detail=""):
    status = "PASS" if passed else "FAIL"
    results.append({"name": name, "status": status, "detail": detail})
    print(f"  {'✅' if passed else '❌'} {name}: {detail}" if detail else f"  {'✅' if passed else '❌'} {name}")

def skip(name, reason):
    results.append({"name": name, "status": "SKIP", "detail": reason})
    print(f"  ⏭️  {name}: SKIP — {reason}")

# =============================================================
# PHASE 0: PREREQUISITES — verify services are reachable
# =============================================================
def test_prerequisites():
    """Check PostgreSQL, Qdrant, Claude API, Dashboard, Slack are reachable."""
    # 0a. PostgreSQL connection
    # 0b. Qdrant health endpoint
    # 0c. Claude API (dry ping — models list or similar)
    # 0d. Dashboard API running (GET /api/status)
    # 0e. Slack webhook configured (check env var exists, don't post yet)

# =============================================================
# PHASE 1: INJECT SYNTHETIC TRIGGER
# =============================================================
def test_trigger_injection():
    """Create a TriggerEvent and run the full pipeline."""
    from orchestrator.pipeline import SentinelPipeline, TriggerEvent

    # Use a real seeded contact so retrieval actually exercises semantic search
    # against existing Qdrant vectors and PostgreSQL contact data.
    trigger = TriggerEvent(
        type="email",
        content=f"{TEST_PREFIX} Christophe Buchwalder sent revised AO agreement "
                f"terms. Key change: liability cap reduced from EUR 2M to EUR 1.5M. "
                f"Buchwalder recommends we counter at EUR 1.8M with 90-day cure period. "
                f"Deadline for response is next Wednesday. Needs CEO sign-off.",
        source_id=f"integration-test-{int(time.time())}",
        contact_name="Christophe Buchwalder",
        metadata={"test": True, "timestamp": datetime.now(timezone.utc).isoformat()}
    )

    pipeline = SentinelPipeline()
    response = pipeline.run(trigger)
    return trigger, response

# =============================================================
# PHASE 2: VALIDATE RETRIEVAL HAPPENED
# =============================================================
def test_retrieval(response):
    """Verify the pipeline retrieved context (non-empty)."""
    # 2a. Response metadata contains retrieval info
    # 2b. At least 1 context was retrieved (even if only from PostgreSQL)

# =============================================================
# PHASE 3: VALIDATE GENERATION OUTPUT
# =============================================================
def test_generation(response):
    """Verify Claude returned a well-formed SentinelResponse."""
    # 3a. response is not None
    # 3b. response has 'analysis' field (non-empty string)
    # 3c. response has 'alerts' field (list)
    # 3d. response has 'decisions_log' field (list)
    # 3e. If alerts exist: each has tier (1-3), title, body
    # 3f. If decisions exist: each has decision, reasoning, confidence

# =============================================================
# PHASE 4: VALIDATE STORE BACK (PostgreSQL)
# =============================================================
def test_store_back(trigger):
    """Verify pipeline results were written to PostgreSQL."""
    from memory.store_back import SentinelStoreBack
    store = SentinelStoreBack()

    # 4a. trigger_log: find entry matching our source_id
    # 4b. decisions: find entries created in last 2 minutes with TEST_PREFIX
    # 4c. alerts: find entries created in last 2 minutes with TEST_PREFIX
    # 4d. no duplicate writes: exactly 1 trigger_log row for our source_id
    # 4e. Qdrant vector stored: search Qdrant for [INTEGRATION-TEST] content,
    #      verify vector created in last 2 minutes (proves learning loop closes)

# =============================================================
# PHASE 5: VALIDATE DASHBOARD API SERVES NEW DATA
# =============================================================
def test_dashboard_api(trigger):
    """Verify the data written by store_back appears in API responses."""
    # 5a. GET /api/status — alerts_pending includes our test alert (if created)
    # 5b. GET /api/alerts — our test alert appears in the list
    # 5c. GET /api/decisions — our test decision appears
    # 5d. GET /api/briefing/latest — endpoint responds (content may not include test data)

# =============================================================
# PHASE 6: VALIDATE SLACK DELIVERY (optional)
# =============================================================
def test_slack_delivery():
    """Verify Slack webhook was called for Tier 1/2 alerts."""
    # 6a. If pipeline created Tier 1/2 alert → Slack notifier should have fired
    # This is observational — check SlackNotifier logs or response
    # If --skip-slack: skip this phase

# =============================================================
# PHASE 7: VALIDATE ALERT ACTIONS (acknowledge/resolve via API)
# =============================================================
def test_alert_actions():
    """Verify the dashboard's alert action endpoints work on test data."""
    # 7a. Find our test alert via GET /api/alerts
    # 7b. POST /api/alerts/{id}/acknowledge — expect 200
    # 7c. POST /api/alerts/{id}/resolve — expect 200
    # 7d. GET /api/alerts — our alert should no longer be pending

# =============================================================
# CLEANUP: REMOVE TEST DATA
# =============================================================
def cleanup_test_data():
    """Remove all test data (rows with TEST_PREFIX) from PostgreSQL."""
    from memory.store_back import SentinelStoreBack
    store = SentinelStoreBack()
    # DELETE FROM trigger_log WHERE source_id LIKE 'integration-test-%'
    # DELETE FROM alerts WHERE title LIKE '[INTEGRATION-TEST]%'
    # DELETE FROM decisions WHERE decision LIKE '[INTEGRATION-TEST]%'
    # Note: Qdrant vectors are not cleaned (they'll be overwritten naturally)

# =============================================================
# MAIN — Run all phases in order
# =============================================================
def main():
    print("\n" + "="*60)
    print("  BAKER INTEGRATION TEST — End-to-End Pipeline")
    print("="*60 + "\n")

    # Parse args
    skip_slack = "--skip-slack" in sys.argv
    cleanup_only = "--cleanup-only" in sys.argv

    if cleanup_only:
        cleanup_test_data()
        return

    # Phase 0: Prerequisites
    print("Phase 0: Prerequisites")
    test_prerequisites()

    # Phase 1: Inject trigger + run pipeline
    print("\nPhase 1: Pipeline Execution")
    trigger, response = test_trigger_injection()

    # Phase 2: Retrieval
    print("\nPhase 2: Retrieval Validation")
    test_retrieval(response)

    # Phase 3: Generation
    print("\nPhase 3: Generation Validation")
    test_generation(response)

    # Phase 4: Store Back
    print("\nPhase 4: Store Back (PostgreSQL)")
    test_store_back(trigger)

    # Phase 5: Dashboard API
    print("\nPhase 5: Dashboard API")
    test_dashboard_api(trigger)

    # Phase 6: Slack (optional)
    if not skip_slack:
        print("\nPhase 6: Slack Delivery")
        test_slack_delivery()
    else:
        print("\nPhase 6: Slack Delivery — SKIPPED (--skip-slack)")

    # Phase 7: Alert Actions
    print("\nPhase 7: Alert Actions (acknowledge/resolve)")
    test_alert_actions()

    # Cleanup
    print("\nCleanup: Removing test data")
    cleanup_test_data()

    # Report
    print("\n" + "="*60)
    passed = sum(1 for r in results if r["status"] == "PASS")
    failed = sum(1 for r in results if r["status"] == "FAIL")
    skipped = sum(1 for r in results if r["status"] == "SKIP")
    total = len(results)
    print(f"  RESULTS: {passed}/{total} PASS | {failed} FAIL | {skipped} SKIP")
    print("="*60 + "\n")

    sys.exit(1 if failed > 0 else 0)

if __name__ == "__main__":
    main()
```

---

## Test Phases Detail

### Phase 0: Prerequisites (5 checks)

| # | Check | How | Pass | Skip if |
|---|-------|-----|------|---------|
| 0a | PostgreSQL reachable | `SentinelStoreBack()` constructor connects | Connection succeeds | — (FAIL = no test possible) |
| 0b | Qdrant reachable | `requests.get(qdrant_url + "/health")` | Status 200 | URL not configured → SKIP retrieval checks |
| 0c | Claude API reachable | Quick `anthropic.Anthropic().models.list()` or similar lightweight call | No exception | API key missing → FAIL |
| 0d | Dashboard running | `requests.get(DASHBOARD_URL + "/api/status")` | Status 200 | Not running → SKIP API checks, print reminder |
| 0e | Slack configured | `os.getenv("SLACK_WEBHOOK_URL")` exists | Non-empty | Missing → SKIP Slack checks |

If PostgreSQL or Claude API are unreachable → **abort entire test** (these are required).

### Phase 1: Pipeline Execution (3 checks)

| # | Check | Pass |
|---|-------|------|
| 1a | `TriggerEvent` created | Object is not None, has all fields |
| 1b | `pipeline.run(trigger)` completes | Returns `SentinelResponse` within TIMEOUT |
| 1c | No unhandled exceptions | Pipeline returns cleanly (even if response is sparse) |

### Phase 2: Retrieval Validation (3 checks)

| # | Check | Pass |
|---|-------|------|
| 2a | Response metadata exists | `response.metadata` is dict |
| 2b | Contexts retrieved | At least 1 `RetrievedContext` returned |
| 2c | PostgreSQL structured data present | Contact profile or deals or alerts in context |

### Phase 3: Generation Validation (5 checks)

| # | Check | Pass |
|---|-------|------|
| 3a | Response not None | `response is not None` |
| 3b | Analysis present | `response.analysis` is non-empty string |
| 3c | Alerts field exists | `response.alerts` is a list |
| 3d | Decisions field exists | `response.decisions_log` is a list |
| 3e | Alert structure valid | Each alert has `tier` (1-3), `title`, `body` |

### Phase 4: Store Back (4 checks)

| # | Check | Pass |
|---|-------|------|
| 4a | Trigger logged | `trigger_log` row exists with our `source_id` |
| 4b | Decisions stored | At least 1 decision row with `[INTEGRATION-TEST]` in last 2 min |
| 4c | Alerts stored | At least 1 alert row with `[INTEGRATION-TEST]` title in last 2 min |
| 4d | No duplicate writes | Exactly 1 trigger_log row for our `source_id` |
| 4e | Vector stored in Qdrant | Search Qdrant for `[INTEGRATION-TEST]` content, find vector created in last 2 min |

### Phase 5: Dashboard API (4 checks)

| # | Check | Pass |
|---|-------|------|
| 5a | `/api/status` reflects test data | `alerts_pending` count ≥ 1 |
| 5b | `/api/alerts` includes test alert | Find alert with `[INTEGRATION-TEST]` title |
| 5c | `/api/decisions` includes test decision | Find decision with `[INTEGRATION-TEST]` prefix |
| 5d | `/api/briefing/latest` responds | Status 200, has `content` field |

### Phase 6: Slack Delivery (2 checks)

| # | Check | Pass |
|---|-------|------|
| 6a | Slack notifier was invoked | Pipeline created Tier 1 or 2 alert → notifier should have fired |
| 6b | No Slack exceptions | No `SlackNotifier` errors in pipeline output |

*Note: We can't verify message appeared in Slack channel without the Slack API (read). This test just verifies the webhook was called without error. Full Slack verification = manual check.*

**Post-test manual step:** After the first successful run with Slack enabled (no `--skip-slack`), manually check the Slack channel for the `[INTEGRATION-TEST]` message. This one-time check confirms the full delivery chain. Add a reminder line to the test output: `"⚠️  MANUAL CHECK: Verify [INTEGRATION-TEST] message appeared in Slack channel."`

### Phase 7: Alert Actions (4 checks)

| # | Check | Pass |
|---|-------|------|
| 7a | Find test alert | `GET /api/alerts` returns our test alert with an `id` |
| 7b | Acknowledge works | `POST /api/alerts/{id}/acknowledge` returns 200 |
| 7c | Resolve works | `POST /api/alerts/{id}/resolve` returns 200 |
| 7d | Alert removed from pending | `GET /api/alerts` no longer includes resolved alert |

### Cleanup

- DELETE test rows from `trigger_log`, `alerts`, `decisions` using `source_id` and `[INTEGRATION-TEST]` prefix
- Qdrant vectors not cleaned (negligible, will be naturally overwritten)
- `--cleanup-only` flag runs only this step (useful if previous run crashed mid-test)

---

## Success Criteria

```
RESULTS: 31/31 PASS | 0 FAIL | 0 SKIP
```

**If all 31 pass:** Baker v1 is operational end-to-end. Every link in the chain — including the learning loop back into Qdrant — is proven.

**Acceptable:** `28/31 PASS | 0 FAIL | 3 SKIP` — where skips are Slack (webhook not configured), Qdrant retrieval (not populated), or Qdrant store-back (service unreachable). The critical path (Trigger → Generation → PostgreSQL Store → API) must be all-PASS.

---

## How to Run

```bash
# Terminal 1: Start the dashboard API
cd /path/to/01_build
uvicorn outputs.dashboard:app --host 0.0.0.0 --port 8080 &

# Terminal 2: Run integration test
python scripts/test_integration.py

# Or skip Slack checks:
python scripts/test_integration.py --skip-slack

# Clean up test data only:
python scripts/test_integration.py --cleanup-only
```

**Environment required:** Same `.env` as production — PostgreSQL DSN, Qdrant URL + API key, Anthropic API key, Slack webhook URL (optional).

**Estimated runtime:** 30–60 seconds (Claude API call is the bottleneck).

**Cost:** ~$0.02–$0.05 (one Claude API call with minimal context retrieval).

---

## What NOT to Build

- ❌ No unit tests (this is integration only — each module was tested in its punch)
- ❌ No mocking — test hits live services
- ❌ No CI/CD pipeline (Baker is a personal system, not a team project)
- ❌ No load testing or stress testing
- ❌ No UI testing (Selenium, Playwright etc.) — the frontend was verified in 5C
- ❌ No pytest framework — simple script with clear output

---

## Dependencies

**Python (all already installed from previous punches):**
- `requests` — for HTTP calls to dashboard API
- `anthropic` — for Claude API prerequisite check
- All Baker modules (`orchestrator`, `memory`, `triggers`, `outputs`)

**No new packages to install.**

---

## Architecture Note

```
┌──────────────────────────────────────────────────────────┐
│                  test_integration.py                      │
│                                                          │
│  Creates TriggerEvent ──▶ SentinelPipeline.run()         │
│                              │                           │
│         ┌────────────────────┼────────────────────┐      │
│         ▼                    ▼                    ▼      │
│   PostgreSQL            Claude API           Qdrant      │
│   (verified)            (verified)         (verified)    │
│         │                    │                           │
│         ▼                    ▼                           │
│   Dashboard API         Slack Webhook                    │
│   (verified)            (verified)                       │
│                                                          │
│  Cleanup: DELETE test rows from PostgreSQL                │
└──────────────────────────────────────────────────────────┘
```

---

## File Checklist

| # | Action | File | ~Lines |
|---|--------|------|--------|
| 1 | CREATE | `scripts/test_integration.py` | ~350 |

**Total new code:** ~350 lines, 1 file.
